{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPXh4Ei8jyLpzFTPI3uW3Ju",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/midnightripper/IDEC/blob/main/Copy_of_idec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCYD333h3l0p",
        "outputId": "e946cd94-9c7e-407d-cb93-e7bacb452233"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.io\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras import backend as K\n",
        "from keras.layers import Layer, InputSpec\n",
        "def dataloader(path):\n",
        "    data = scipy.io.loadmat(path)\n",
        "    AF = data['AF']\n",
        "    modified_rows1 = AF[:-2]\n",
        "    last_rows1 = AF[-2:]\n",
        "    CF = data['CF']\n",
        "    concatenated_array = np.concatenate((modified_rows1, CF,last_rows1), axis=0)\n",
        "    new=concatenated_array\n",
        "    u, count = np.unique(new[-1], return_counts=True)\n",
        "    a = u[np.logical_or(count < 2, count > 2)]\n",
        "    c = new[:, np.isin(new[-1], a, invert=True)]\n",
        "    x = c[0:-2]; y = c[-2]; w = c[-1];\n",
        "    return x.T, y.T, w.T, data['CF_info']\n",
        "\n",
        "def normalization(feats):\n",
        "    df = pd.DataFrame(feats)\n",
        "    scaler = StandardScaler()\n",
        "    x_new = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
        "    return x_new"
      ],
      "metadata": {
        "id": "FQTDkISq4BC3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "original_dim = 38\n",
        "\n",
        "# train_path = filee; test_path = filee.replace('train','test')\n",
        "train_path='/content/drive/MyDrive/finalData/TypicalFA_comb1/GER_train_fisher-2000_FA_GT_ESTphnTrans_estStress.mat'\n",
        "test_path='/content/drive/MyDrive/finalData/TypicalFA_comb1/GER_test_fisher-2000_FA_GT_ESTphnTrans_estStress.mat'\n",
        "x, y, wtrain1, info_train1 = dataloader(train_path);\n",
        "xtest, ytest, wtest1, info_test1 = dataloader(test_path);\n",
        "xtest= normalization(xtest)\n",
        "x= normalization(x)"
      ],
      "metadata": {
        "id": "jaJgtEmjmV2J"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AgAyKCAU3OMx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.layers import Dense, Input\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "import csv, os\n",
        "\n",
        "def cluster_acc(y_true, y_pred):\n",
        "    y_true = y_true.astype(np.int64)\n",
        "    assert y_pred.size == y_true.size\n",
        "    D = max(y_pred.max(), y_true.max()) + 1\n",
        "    w = np.zeros((D, D), dtype=np.int64)\n",
        "    for i in range(y_pred.size):\n",
        "        w[y_pred[i], y_true[i]] += 1\n",
        "    _, col_ind = linear_sum_assignment(w.max() - w)\n",
        "    return sum([w[i, j] for i, j in zip(range(w.shape[0]), col_ind)]) * 1.0 / y_pred.size\n",
        "\n",
        "def autoencoder(dims, act='relu'):\n",
        "    n_stacks = len(dims) - 1\n",
        "    x = Input(shape=(dims[0],), name='input')\n",
        "    h = x\n",
        "    for i in range(n_stacks-1):\n",
        "        h = Dense(dims[i + 1], activation=act, name='encoder_%d' % i)(h)\n",
        "    h = Dense(dims[-1], name='encoder_%d' % (n_stacks - 1))(h)\n",
        "    for i in range(n_stacks-1, 0, -1):\n",
        "        h = Dense(dims[i], activation=act, name='decoder_%d' % i)(h)\n",
        "    h = Dense(dims[0], name='decoder_0')(h)\n",
        "    return Model(inputs=x, outputs=h)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ClusteringLayer(Dense):\n",
        "    def __init__(self, n_clusters, alpha=1.0, **kwargs):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.alpha = alpha\n",
        "        super(ClusteringLayer, self).__init__(n_clusters, **kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 2\n",
        "        input_dim = input_shape[1]\n",
        "        self.input_spec = InputSpec(dtype=tf.float32, shape=(None, input_dim))\n",
        "        self.clusters = self.add_weight(shape=(self.n_clusters, input_dim),\n",
        "                                        initializer='glorot_uniform', name='clusters')\n",
        "        self.built = True\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(ClusteringLayer, self).get_config()\n",
        "        config['n_clusters'] = self.n_clusters\n",
        "        config['alpha'] = self.alpha\n",
        "        return config\n",
        "\n",
        "    def call(self, inputs):\n",
        "        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
        "        q **= (self.alpha + 1.0) / 2.0\n",
        "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1))\n",
        "        return q"
      ],
      "metadata": {
        "id": "G2vMCGxflr5l"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.layers import Dense, Input\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "import csv, os\n",
        "\n",
        "def cluster_acc(y_true, y_pred):\n",
        "    y_true = y_true.astype(np.int64)\n",
        "    assert y_pred.size == y_true.size\n",
        "    D = max(y_pred.max(), y_true.max()) + 1\n",
        "    w = np.zeros((D, D), dtype=np.int64)\n",
        "    for i in range(y_pred.size):\n",
        "        w[y_pred[i], y_true[i]] += 1\n",
        "    _, col_ind = linear_sum_assignment(w.max() - w)\n",
        "    return sum([w[i, j] for i, j in zip(range(w.shape[0]), col_ind)]) * 1.0 / y_pred.size\n",
        "\n",
        "def autoencoder(dims, act='relu'):\n",
        "    n_stacks = len(dims) - 1\n",
        "    x = Input(shape=(dims[0],), name='input')\n",
        "    h = x\n",
        "    for i in range(n_stacks-1):\n",
        "        h = Dense(dims[i + 1], activation=act, name='encoder_%d' % i)(h)\n",
        "    h = Dense(dims[-1], name='encoder_%d' % (n_stacks - 1))(h)\n",
        "    for i in range(n_stacks-1, 0, -1):\n",
        "        h = Dense(dims[i], activation=act, name='decoder_%d' % i)(h)\n",
        "    h = Dense(dims[0], name='decoder_0')(h)\n",
        "    return Model(inputs=x, outputs=h)\n",
        "\n",
        "class ClusteringLayer(Dense):\n",
        "    def __init__(self, n_clusters, alpha=1.0, **kwargs):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.alpha = alpha\n",
        "        super(ClusteringLayer, self).__init__(n_clusters, **kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 2\n",
        "        input_dim = input_shape[1]\n",
        "        self.input_spec = InputSpec(dtype=tf.float32, shape=(None, input_dim))\n",
        "        self.clusters = self.add_weight(shape=(self.n_clusters, input_dim),\n",
        "                                        initializer='glorot_uniform', name='clusters')\n",
        "        self.built = True\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(ClusteringLayer, self).get_config()\n",
        "        config['n_clusters'] = self.n_clusters\n",
        "        config['alpha'] = self.alpha\n",
        "        return config\n",
        "\n",
        "    def call(self, inputs):\n",
        "        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
        "        q **= (self.alpha + 1.0) / 2.0\n",
        "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1))\n",
        "        return q\n",
        "\n",
        "class IDEC(object):\n",
        "    def __init__(self, dims, n_clusters=2, alpha=1.0):\n",
        "        self.dims = dims\n",
        "        self.input_dim = dims[0]\n",
        "        self.n_stacks = len(self.dims) - 1\n",
        "        self.n_clusters = n_clusters\n",
        "        self.alpha = alpha\n",
        "        self.autoencoder = autoencoder(self.dims)\n",
        "\n",
        "    def initialize_model(self, ae_weights=None, gamma=0.1, optimizer='adam'):\n",
        "        if ae_weights is not None:\n",
        "            self.autoencoder.load_weights(ae_weights)\n",
        "            print('Pretrained AE weights are loaded successfully.')\n",
        "        else:\n",
        "            print('Training the autoencoder from scratch.')\n",
        "            self.autoencoder.compile(loss='mse', optimizer=optimizer)\n",
        "            self.autoencoder.fit(x, x, batch_size=self.batch_size, epochs=200)  # You can adjust the epochs as needed\n",
        "\n",
        "        hidden = self.autoencoder.get_layer(name='encoder_%d' % (self.n_stacks - 1)).output\n",
        "        self.encoder = Model(inputs=self.autoencoder.input, outputs=hidden)\n",
        "\n",
        "        # prepare IDEC model\n",
        "        clustering_layer = ClusteringLayer(self.n_clusters, name='clustering')(hidden)\n",
        "        self.model = Model(inputs=self.autoencoder.input,\n",
        "                           outputs=[clustering_layer, self.autoencoder.output])\n",
        "        self.model.compile(loss={'clustering': 'kld', 'decoder_0': 'mse'},\n",
        "                           loss_weights=[gamma, 1],\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "    def fit(self, x, y=None, batch_size=256, maxiter=2e4, tol=1e-3, update_interval=140, save_dir='./results/idec'):\n",
        "        print('Update interval', update_interval)\n",
        "        save_interval = int(x.shape[0] / batch_size) * 5\n",
        "        print('Save interval', save_interval)\n",
        "        kmeans = KMeans(n_clusters=self.n_clusters, n_init=20)\n",
        "        y_pred = kmeans.fit_predict(self.encoder.predict(x))\n",
        "        y_pred_last = np.copy(y_pred)\n",
        "        self.model.get_layer(name='clustering').set_weights([kmeans.cluster_centers_])\n",
        "\n",
        "        if not os.path.exists(save_dir):\n",
        "            os.makedirs(save_dir)\n",
        "        logfile = open(save_dir + '/idec_log.csv', 'w')\n",
        "        logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'acc', 'nmi', 'ari', 'L'])\n",
        "        logwriter.writeheader()\n",
        "\n",
        "        loss = [0, 0, 0]\n",
        "        index = 0\n",
        "        for ite in range(int(maxiter)):\n",
        "            if ite % update_interval == 0:\n",
        "                q, _ = self.model.predict(x, verbose=0)\n",
        "                p = self.target_distribution(q)\n",
        "                y_pred = q.argmax(1)\n",
        "                if y is not None:\n",
        "                    acc = cluster_acc(y, y_pred)\n",
        "                    nmi = normalized_mutual_info_score(y, y_pred)\n",
        "                    ari = adjusted_rand_score(y, y_pred)\n",
        "                    loss = np.round(loss, 5)\n",
        "                    logwriter.writerow({'iter': ite, 'acc': acc, 'nmi': nmi, 'ari': ari, 'L': loss[0]})\n",
        "                    print('Iter', ite, ': Acc', acc, ', nmi', nmi, ', ari', ari, '; loss=', loss)\n",
        "\n",
        "                delta_label = np.sum(y_pred != y_pred_last).astype(np.float32) / y_pred.shape[0]\n",
        "                y_pred_last = np.copy(y_pred)\n",
        "\n",
        "                if ite > 0 and delta_label < tol:\n",
        "                    print('delta_label ', delta_label, '< tol ', tol)\n",
        "                    print('Reached tolerance threshold. Stopping training.')\n",
        "                    logfile.close()\n",
        "                    break\n",
        "\n",
        "            if (index + 1) * batch_size > x.shape[0]:\n",
        "                loss = self.model.train_on_batch(x=x[index * batch_size::], y=[p[index * batch_size::], x[index * batch_size::]])\n",
        "                index = 0\n",
        "            else:\n",
        "                loss = self.model.train_on_batch(x=x[index * batch_size:(index + 1) * batch_size],\n",
        "                                                 y=[p[index * batch_size:(index + 1) * batch_size],\n",
        "                                                    x[index * batch_size:(index + 1) * batch_size]])\n",
        "                index += 1\n",
        "\n",
        "            if ite % save_interval == 0:\n",
        "                print('saving model to:', save_dir + '/IDEC_model_' + str(ite) + '.h5')\n",
        "                self.model.save_weights(save_dir + '/IDEC_model_' + str(ite) + '.h5')\n",
        "\n",
        "            ite += 1\n",
        "\n",
        "        logfile.close()\n",
        "        print('saving model to:', save_dir + '/IDEC_model_final.h5')\n",
        "        self.model.save_weights(save_dir + '/IDEC_model_final.h5')\n",
        "\n",
        "        return y_pred\n",
        "\n",
        "    def predict_clusters(self, x):\n",
        "        q, _ = self.model.predict(x, verbose=0)\n",
        "        return q.argmax(axis=1)\n",
        "\n",
        "    @staticmethod\n",
        "    def target_distribution(q):\n",
        "        weight = q ** 2 / q.sum(0)\n",
        "        return (weight.T / weight.sum(1)).T\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Implement the rest of your code here using the modified DEC and IDEC classes.\n",
        "\n",
        "    # Example usage:\n",
        "\n",
        "    # load dataset and preprocess (implement dataloader and normalization functions)\n",
        "    x, y, wtrain1, info_train1 = dataloader(train_path)\n",
        "    xtest, ytest, wtest1, info_test1 = dataloader(test_path)\n",
        "    xtest = normalization(xtest)\n",
        "    x = normalization(x)\n",
        "\n",
        "    # Create and train the DEC model\n",
        "    # dec = DEC(dims=[x.shape[-1], 500, 500, 2000, 10], n_clusters=2)\n",
        "    # dec.pretrain(x, batch_size=256, epochs=200, optimizer='adam')\n",
        "    # dec.compile(loss='kld', optimizer='adam')\n",
        "    # dec.fit(x, y=y, batch_size=256, tol=0.001, maxiter=20000, update_interval=140, save_dir='./results/dec')\n",
        "    # Load the autoencoder weights\n",
        "    # Assuming you have already loaded the dataset `x` with shape (5000, 38) and labels `y`\n",
        "\n",
        "    # Create and train the IDEC model\n",
        "    idec = IDEC(dims=[x.shape[-1], 500, 500, 2000, 10], n_clusters=2)\n",
        "    idec.initialize_model(ae_weights='autoencoder_weights.h5', gamma=0.1, optimizer='adam')\n",
        "    idec.fit(x, y=y, batch_size=256, tol=0.001, maxiter=20000, update_interval=140, save_dir='./results/idec')\n",
        "\n",
        "    # Show the final results\n",
        "    y_pred = idec.predict_clusters(x)\n",
        "    print('ACC:', cluster_acc(y, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXi0hnc2YqNd",
        "outputId": "e150e43a-fa6b-43b5-95e1-4560f3870722"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pretrained AE weights are loaded successfully.\n",
            "Update interval 140\n",
            "Save interval 75\n",
            "124/124 [==============================] - 1s 1ms/step\n",
            "Iter 0 : Acc 0.5244702320887992 , nmi 0.001732987762655746 , ari 0.002143861061730856 ; loss= [0 0 0]\n",
            "saving model to: ./results/idec/IDEC_model_0.h5\n",
            "saving model to: ./results/idec/IDEC_model_75.h5\n",
            "Iter 140 : Acc 0.5227043390514632 , nmi 0.0014893986780975656 , ari 0.0018102577153003472 ; loss= [0.0213  0.00527 0.02077]\n",
            "saving model to: ./results/idec/IDEC_model_150.h5\n",
            "saving model to: ./results/idec/IDEC_model_225.h5\n",
            "Iter 280 : Acc 0.5216952573158425 , nmi 0.001362025781752033 , ari 0.0016312886030698712 ; loss= [0.02531 0.01188 0.02412]\n",
            "saving model to: ./results/idec/IDEC_model_300.h5\n",
            "saving model to: ./results/idec/IDEC_model_375.h5\n",
            "Iter 420 : Acc 0.5229566094853683 , nmi 0.0015281564100230992 , ari 0.0018570060961422743 ; loss= [0.03061 0.0175  0.02886]\n",
            "saving model to: ./results/idec/IDEC_model_450.h5\n",
            "saving model to: ./results/idec/IDEC_model_525.h5\n",
            "Iter 560 : Acc 0.5211907164480323 , nmi 0.0013000436567954265 , ari 0.0015448082713883878 ; loss= [0.02427 0.02839 0.02143]\n",
            "saving model to: ./results/idec/IDEC_model_600.h5\n",
            "saving model to: ./results/idec/IDEC_model_675.h5\n",
            "Iter 700 : Acc 0.5199293642785066 , nmi 0.0011508871359609872 , ari 0.001337456075278218 ; loss= [0.04351 0.0357  0.03994]\n",
            "saving model to: ./results/idec/IDEC_model_750.h5\n",
            "saving model to: ./results/idec/IDEC_model_825.h5\n",
            "Iter 840 : Acc 0.5199293642785066 , nmi 0.00114933649458572 , ari 0.0013372071158921641 ; loss= [0.03626 0.03721 0.03254]\n",
            "saving model to: ./results/idec/IDEC_model_900.h5\n",
            "saving model to: ./results/idec/IDEC_model_975.h5\n",
            "Iter 980 : Acc 0.5199293642785066 , nmi 0.00114933649458572 , ari 0.0013372071158921641 ; loss= [0.02871 0.03876 0.02484]\n",
            "saving model to: ./results/idec/IDEC_model_1050.h5\n",
            "Iter 1120 : Acc 0.5191725529767911 , nmi 0.001063132984776259 , ari 0.0012187109092381423 ; loss= [0.02352 0.04515 0.019  ]\n",
            "saving model to: ./results/idec/IDEC_model_1125.h5\n",
            "saving model to: ./results/idec/IDEC_model_1200.h5\n",
            "Iter 1260 : Acc 0.5191725529767911 , nmi 0.0010645800356906953 , ari 0.001218962208394369 ; loss= [0.02853 0.04362 0.02417]\n",
            "saving model to: ./results/idec/IDEC_model_1275.h5\n",
            "saving model to: ./results/idec/IDEC_model_1350.h5\n",
            "Iter 1400 : Acc 0.5171543895055499 , nmi 0.00085245326965834 , ari 0.0009256859266818615 ; loss= [0.03297 0.04394 0.02858]\n",
            "saving model to: ./results/idec/IDEC_model_1425.h5\n",
            "saving model to: ./results/idec/IDEC_model_1500.h5\n",
            "Iter 1540 : Acc 0.515640766902119 , nmi 0.0007087367211423049 , ari 0.0007271049547035711 ; loss= [0.03356 0.04371 0.02919]\n",
            "saving model to: ./results/idec/IDEC_model_1575.h5\n",
            "saving model to: ./results/idec/IDEC_model_1650.h5\n",
            "Iter 1680 : Acc 0.5146316851664985 , nmi 0.0006203162481113764 , ari 0.0006049121713437165 ; loss= [0.02232 0.04196 0.01813]\n",
            "saving model to: ./results/idec/IDEC_model_1725.h5\n",
            "saving model to: ./results/idec/IDEC_model_1800.h5\n",
            "Iter 1820 : Acc 0.515640766902119 , nmi 0.0007086332942937815 , ari 0.0007270779890267941 ; loss= [0.03086 0.04558 0.0263 ]\n",
            "saving model to: ./results/idec/IDEC_model_1875.h5\n",
            "saving model to: ./results/idec/IDEC_model_1950.h5\n",
            "Iter 1960 : Acc 0.5153884964682139 , nmi 0.0006858083194885204 , ari 0.0006957195583165074 ; loss= [0.02747 0.04012 0.02345]\n",
            "saving model to: ./results/idec/IDEC_model_2025.h5\n",
            "Iter 2100 : Acc 0.5153884964682139 , nmi 0.0006860046882250327 , ari 0.0006957724640670979 ; loss= [0.03235 0.0487  0.02748]\n",
            "saving model to: ./results/idec/IDEC_model_2100.h5\n",
            "saving model to: ./results/idec/IDEC_model_2175.h5\n",
            "Iter 2240 : Acc 0.5146316851664985 , nmi 0.0006199577233705261 , ari 0.0006048053228308645 ; loss= [0.02128 0.03594 0.01769]\n",
            "saving model to: ./results/idec/IDEC_model_2250.h5\n",
            "saving model to: ./results/idec/IDEC_model_2325.h5\n",
            "Iter 2380 : Acc 0.5161453077699294 , nmi 0.0007546775419728256 , ari 0.0007911409106182935 ; loss= [0.03255 0.0424  0.02831]\n",
            "saving model to: ./results/idec/IDEC_model_2400.h5\n",
            "saving model to: ./results/idec/IDEC_model_2475.h5\n",
            "Iter 2520 : Acc 0.5146316851664985 , nmi 0.0006198724525387023 , ari 0.0006047798949362194 ; loss= [0.02029 0.03578 0.01671]\n",
            "saving model to: ./results/idec/IDEC_model_2550.h5\n",
            "saving model to: ./results/idec/IDEC_model_2625.h5\n",
            "Iter 2660 : Acc 0.5151362260343088 , nmi 0.0006634573275768063 , ari 0.0006648969584000963 ; loss= [0.0268  0.0366  0.02314]\n",
            "saving model to: ./results/idec/IDEC_model_2700.h5\n",
            "saving model to: ./results/idec/IDEC_model_2775.h5\n",
            "Iter 2800 : Acc 0.5143794147325933 , nmi 0.0005987198149853491 , ari 0.0005755106032361746 ; loss= [0.02105 0.03644 0.01741]\n",
            "delta_label  0.0007568113017154389 < tol  0.001\n",
            "Reached tolerance threshold. Stopping training.\n",
            "saving model to: ./results/idec/IDEC_model_final.h5\n",
            "ACC: 0.5143794147325933\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(arr1, arr2):\n",
        "    count = sum(1 for itr1, itr2 in zip(arr1, arr2) if itr1 == itr2)\n",
        "    return count / len(arr1)"
      ],
      "metadata": {
        "id": "Tk5LxVLl4tp4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(calculate_accuracy(y,y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_s4OILOcCXy7",
        "outputId": "6b4544db-1b95-41bb-984e-eba2eeaee972"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5143794147325933\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def flip(arr):\n",
        "  result = []\n",
        "  for num in arr:\n",
        "      if num == 1:\n",
        "          result.append(0)\n",
        "      else:\n",
        "          result.append(1)\n",
        "  return result"
      ],
      "metadata": {
        "id": "bjL0RFFnCb2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_flip=flip(y_pred)"
      ],
      "metadata": {
        "id": "0N-7WPnnDRB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(calculate_accuracy(y,y_pred_flip))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JS7TlHyiDWa-",
        "outputId": "4c714716-99f6-4951-d167-c9eed069ac12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4760343087790111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ua7urQtDZEx",
        "outputId": "5d48afd7-ba7d-4326-be54-0f34a12726a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 0 ... 1 1 1]\n"
          ]
        }
      ]
    }
  ]
}