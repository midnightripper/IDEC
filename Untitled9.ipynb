{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAYu8UTWrz0f",
        "outputId": "f2238203-bc82-47d8-dbd9-adb48492786c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "\n",
        "from keras.layers import Dense, Input\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "# Load the MNIST dataset\n",
        "from keras.datasets import mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Reshape data to have shape (samples, dim)\n",
        "x_train = x_train.reshape(x_train.shape[0], -1)\n",
        "x_test = x_test.reshape(x_test.shape[0], -1)\n",
        "\n",
        "# Define model parameters\n",
        "dims = [x_train.shape[-1], 500, 500, 2000, 10]\n",
        "n_clusters = 10\n",
        "batch_size = 256\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define autoencoder model\n",
        "def autoencoder(dims, act='relu'):\n",
        "    def __init__(self, dims, act='relu'):\n",
        "      self.dims = dims\n",
        "      self.n_stacks = len(self.dims) - 1\n",
        "\n",
        "    def __call__(self):\n",
        "      input_layer = Input(shape=(self.dims[0],))\n",
        "      for dim in self.dims[1:-1]:\n",
        "          hidden_layer = Dense(dim, activation=act)(input_layer)\n",
        "      latent_layer = Dense(self.dims[-1])(hidden_layer)\n",
        "      for dim in self.dims[-2::-1]:\n",
        "          hidden_layer = Dense(dim, activation=act)(latent_layer)\n",
        "      output_layer = Dense(self.dims[0])(hidden_layer)\n",
        "      return Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Define ClusteringLayer\n",
        "class ClusteringLayer(Layer):\n",
        "    def __init__(self, n_clusters, **kwargs):\n",
        "        self.n_clusters = n_clusters\n",
        "        super(ClusteringLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.input_spec = InputSpec(shape=input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.cluster_centers_), axis=2) / self.alpha))\n",
        "        q **= (self.alpha + 1.0) / 2.0\n",
        "        q = K.pow(q, self.alpha)\n",
        "        q /= K.sum(q, axis=1, keepdims=True)\n",
        "        return q\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], self.n_clusters)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'n_clusters': self.n_clusters}\n",
        "        base_config = super(ClusteringLayer, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "\n",
        "# Define DEC model\n",
        "class DEC():\n",
        "    ...\n",
        "\n",
        "# Initialize DEC model\n",
        "dec = DEC(dims, n_clusters, batch_size)\n",
        "dec.initialize_model(optimizer=SGD(lr=0.01, momentum=0.9))\n",
        "\n",
        "# Train model\n",
        "t0 = time()\n",
        "y_pred = dec.clustering(x_train, y_train, ...)\n",
        "print('Training time: ', time() - t0)\n",
        "\n",
        "# Evaluate clustering performance\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "print('ARI: ', adjusted_rand_score(y_train, y_pred))\n"
      ],
      "metadata": {
        "id": "lg6oxns8sA6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from time import time\n",
        "import numpy as np\n",
        "import keras.backend as K\n",
        "from keras.engine.topology import Layer, InputSpec\n",
        "from keras.layers import Dense, Input\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn import metrics\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "EfQva1Z1uvjJ",
        "outputId": "492722af-4aab-428e-c92f-b75dc7913c51"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-a4c327fc6f9c>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopology\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInputSpec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.engine.topology'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def cluster_acc(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate clustering accuracy. Require scikit-learn installed.\n",
        "\n",
        "    # Arguments\n",
        "        y: true labels, numpy.array with shape `(n_samples,)`\n",
        "        y_pred: predicted labels, numpy.array with shape `(n_samples,)`\n",
        "\n",
        "    # Return\n",
        "        accuracy, in [0,1]\n",
        "    \"\"\"\n",
        "    y_true = y_true.astype(np.int64)\n",
        "    assert y_pred.size == y_true.size\n",
        "    D = max(y_pred.max(), y_true.max()) + 1\n",
        "    w = np.zeros((D, D), dtype=np.int64)\n",
        "    for i in range(y_pred.size):\n",
        "        w[y_pred[i], y_true[i]] += 1\n",
        "    from sklearn.utils.linear_assignment_ import linear_assignment\n",
        "    ind = linear_assignment(w.max() - w)\n",
        "    return sum([w[i, j] for i, j in ind]) * 1.0 / y_pred.size\n",
        "\n",
        "\n",
        "def autoencoder(dims, act='relu'):\n",
        "    \"\"\"\n",
        "    Fully connected auto-encoder model, symmetric.\n",
        "    Arguments:\n",
        "        dims: list of number of units in each layer of encoder. dims[0] is input dim, dims[-1] is units in hidden layer.\n",
        "            The decoder is symmetric with encoder. So number of layers of the auto-encoder is 2*len(dims)-1\n",
        "        act: activation, not applied to Input, Hidden and Output layers\n",
        "    return:\n",
        "        Model of autoencoder\n",
        "    \"\"\"\n",
        "    n_stacks = len(dims) - 1\n",
        "    # input\n",
        "    x = Input(shape=(dims[0],), name='input')\n",
        "    h = x\n",
        "\n",
        "    # internal layers in encoder\n",
        "    for i in range(n_stacks-1):\n",
        "        h = Dense(dims[i + 1], activation=act, name='encoder_%d' % i)(h)\n",
        "\n",
        "    # hidden layer\n",
        "    h = Dense(dims[-1], name='encoder_%d' % (n_stacks - 1))(h)  # hidden layer, features are extracted from here\n",
        "\n",
        "    # internal layers in decoder\n",
        "    for i in range(n_stacks-1, 0, -1):\n",
        "        h = Dense(dims[i], activation=act, name='decoder_%d' % i)(h)\n",
        "\n",
        "    # output\n",
        "    h = Dense(dims[0], name='decoder_0')(h)\n",
        "\n",
        "    return Model(inputs=x, outputs=h)\n",
        "\n",
        "\n",
        "class ClusteringLayer(Layer):\n",
        "    \"\"\"\n",
        "    Clustering layer converts input sample (feature) to soft label, i.e. a vector that represents the probability of the\n",
        "    sample belonging to each cluster. The probability is calculated with student's t-distribution.\n",
        "\n",
        "    # Example\n",
        "    # Arguments\n",
        "        n_clusters: number of clusters.\n",
        "        weights: list of Numpy array with shape `(n_clusters, n_features)` witch represents the initial cluster centers.\n",
        "        alpha: parameter in Student's t-distribution. Default to 1.0.\n",
        "    # Input shape\n",
        "        2D tensor with shape: `(n_samples, n_features)`.\n",
        "    # Output shape\n",
        "        2D tensor with shape: `(n_samples, n_clusters)`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n",
        "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
        "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
        "        super(ClusteringLayer, self).__init__(**kwargs)\n",
        "        self.n_clusters = n_clusters\n",
        "        self.alpha = alpha\n",
        "        self.initial_weights = weights\n",
        "        self.input_spec = InputSpec(ndim=2)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 2\n",
        "        input_dim = input_shape[1]\n",
        "        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
        "        self.clusters = self.add_weight((self.n_clusters, input_dim), initializer='glorot_uniform', name='clusters')\n",
        "        if self.initial_weights is not None:\n",
        "            self.set_weights(self.initial_weights)\n",
        "            del self.initial_weights\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        \"\"\" student t-distribution, as same as used in t-SNE algorithm.\n",
        "                 q_ij = 1/(1+dist(x_i, u_j)^2), then normalize it.\n",
        "        Arguments:\n",
        "            inputs: the variable containing data, shape=(n_samples, n_features)\n",
        "        Return:\n",
        "            q: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n",
        "        \"\"\"\n",
        "        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
        "        q **= (self.alpha + 1.0) / 2.0\n",
        "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1))\n",
        "        return q\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        assert input_shape and len(input_shape) == 2\n",
        "        return input_shape[0], self.n_clusters\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'n_clusters': self.n_clusters}\n",
        "        base_config = super(ClusteringLayer, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "\n",
        "class DEC(object):\n",
        "    def __init__(self,\n",
        "                 dims,\n",
        "                 n_clusters=10,\n",
        "                 alpha=1.0,\n",
        "                 batch_size=256):\n",
        "\n",
        "        super(DEC, self).__init__()\n",
        "\n",
        "        self.dims = dims\n",
        "        self.input_dim = dims[0]\n",
        "        self.n_stacks = len(self.dims) - 1\n",
        "\n",
        "        self.n_clusters = n_clusters\n",
        "        self.alpha = alpha\n",
        "        self.batch_size = batch_size\n",
        "        self.autoencoder = autoencoder(self.dims)\n",
        "\n",
        "    def initialize_model(self, optimizer, ae_weights=None):\n",
        "        if ae_weights is not None: # load pretrained weights of autoencoder\n",
        "            self.autoencoder.load_weights(ae_weights)\n",
        "        else:\n",
        "            print 'ae_weights must be given. E.g.'\n",
        "            print 'python DEC.py mnist --ae_weights weights.h5'\n",
        "            exit()\n",
        "\n",
        "        hidden = self.autoencoder.get_layer(name='encoder_%d' % (self.n_stacks - 1)).output\n",
        "        self.encoder = Model(inputs=self.autoencoder.input, outputs=hidden)\n",
        "\n",
        "        # prepare DEC model\n",
        "        clustering_layer = ClusteringLayer(self.n_clusters, name='clustering')(hidden)\n",
        "        self.model = Model(inputs=self.autoencoder.input, outputs=clustering_layer)\n",
        "        self.model.compile(loss='kld', optimizer=optimizer)\n",
        "\n",
        "    def load_weights(self, weights_path): # load weights of DEC model\n",
        "        self.model.load_weights(weights_path)\n",
        "\n",
        "    def extract_feature(self, x): # extract features from before clustering layer\n",
        "        encoder = Model(self.model.input, self.model.get_layer('encoder_%d' % (self.n_stacks - 1)).output)\n",
        "        return encoder.predict(x)\n",
        "\n",
        "    def predict_clusters(self, x): # predict cluster labels using the output of clustering layer\n",
        "        q = self.model.predict(x, verbose=0)\n",
        "        return q.argmax(1)\n",
        "\n",
        "    @staticmethod\n",
        "    def target_distribution(q):\n",
        "        weight = q ** 2 / q.sum(0)\n",
        "        return (weight.T / weight.sum(1)).T\n",
        "\n",
        "    def clustering(self, x, y=None,\n",
        "                   tol=1e-3,\n",
        "                   update_interval=140,\n",
        "                   maxiter=2e4,\n",
        "                   save_dir='./results/dec'):\n",
        "\n",
        "        print 'Update interval', update_interval\n",
        "        save_interval = x.shape[0] / self.batch_size * 5  # 5 epochs\n",
        "        print 'Save interval', save_interval\n",
        "\n",
        "        # initialize cluster centers using k-means\n",
        "        print 'Initializing cluster centers with k-means.'\n",
        "        kmeans = KMeans(n_clusters=self.n_clusters, n_init=20)\n",
        "        y_pred = kmeans.fit_predict(self.encoder.predict(x))\n",
        "        y_pred_last = y_pred\n",
        "        self.model.get_layer(name='clustering').set_weights([kmeans.cluster_centers_])\n",
        "\n",
        "        # logging file\n",
        "        import csv, os\n",
        "        if not os.path.exists(save_dir):\n",
        "            os.makedirs(save_dir)\n",
        "\n",
        "        logfile = file(save_dir + '/dec_log.csv', 'wb')\n",
        "        logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'acc', 'nmi', 'ari', 'L'])\n",
        "        logwriter.writeheader()\n",
        "\n",
        "        loss = 0\n",
        "        index = 0\n",
        "        for ite in range(int(maxiter)):\n",
        "            if ite % update_interval == 0:\n",
        "                q = self.model.predict(x, verbose=0)\n",
        "                p = self.target_distribution(q)  # update the auxiliary target distribution p\n",
        "\n",
        "                # evaluate the clustering performance\n",
        "                y_pred = q.argmax(1)\n",
        "                delta_label = np.sum(y_pred != y_pred_last).astype(np.float32) / y_pred.shape[0]\n",
        "                y_pred_last = y_pred\n",
        "                if y is not None:\n",
        "                    acc = np.round(cluster_acc(y, y_pred), 5)\n",
        "                    nmi = np.round(metrics.normalized_mutual_info_score(y, y_pred), 5)\n",
        "                    ari = np.round(metrics.adjusted_rand_score(y, y_pred), 5)\n",
        "                    loss = np.round(loss, 5)\n",
        "                    logdict = dict(iter=ite, acc=acc, nmi=nmi, ari=ari, L=loss)\n",
        "                    logwriter.writerow(logdict)\n",
        "                    print 'Iter', ite, ': Acc', acc, ', nmi', nmi, ', ari', ari, '; loss=', loss\n",
        "\n",
        "                # check stop criterion\n",
        "                if ite > 0 and delta_label < tol:\n",
        "                    print 'delta_label ', delta_label, '< tol ', tol\n",
        "                    print 'Reached tolerance threshold. Stopping training.'\n",
        "                    logfile.close()\n",
        "                    break\n",
        "\n",
        "            # train on batch\n",
        "            if (index + 1) * self.batch_size > x.shape[0]:\n",
        "                loss = self.model.train_on_batch(x=x[index * self.batch_size::],\n",
        "                                                 y=p[index * self.batch_size::])\n",
        "                index = 0\n",
        "            else:\n",
        "                loss = self.model.train_on_batch(x=x[index * self.batch_size:(index + 1) * self.batch_size],\n",
        "                                                 y=p[index * self.batch_size:(index + 1) * self.batch_size])\n",
        "                index += 1\n",
        "\n",
        "            # save intermediate model\n",
        "            if ite % save_interval == 0:\n",
        "                # save IDEC model checkpoints\n",
        "                print 'saving model to:', save_dir + '/DEC_model_' + str(ite) + '.h5'\n",
        "                self.model.save_weights(save_dir + '/DEC_model_' + str(ite) + '.h5')\n",
        "\n",
        "            ite += 1\n",
        "\n",
        "        # save the trained model\n",
        "        logfile.close()\n",
        "        print 'saving model to:', save_dir + '/DEC_model_final.h5'\n",
        "        self.model.save_weights(save_dir + '/DEC_model_final.h5')\n",
        "\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # setting the hyper parameters\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser(description='train',\n",
        "                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
        "    parser.add_argument('dataset', default='mnist', choices=['mnist', 'usps', 'reutersidf10k'])\n",
        "    parser.add_argument('--n_clusters', default=10, type=int)\n",
        "    parser.add_argument('--batch_size', default=256, type=int)\n",
        "    parser.add_argument('--maxiter', default=2e4, type=int)\n",
        "    parser.add_argument('--gamma', default=0.1, type=float,\n",
        "                        help='coefficient of clustering loss')\n",
        "    parser.add_argument('--update_interval', default=140, type=int)\n",
        "    parser.add_argument('--tol', default=0.001, type=float)\n",
        "    parser.add_argument('--ae_weights', default=None, help='This argument must be given')\n",
        "    parser.add_argument('--save_dir', default='results/dec')\n",
        "    args = parser.parse_args()\n",
        "    print args\n",
        "\n",
        "    # load dataset\n",
        "    from datasets import load_mnist, load_reuters, load_usps\n",
        "    if args.dataset == 'mnist':  # recommends: n_clusters=10, update_interval=140\n",
        "        x, y = load_mnist()\n",
        "    elif args.dataset == 'usps':  # recommends: n_clusters=10, update_interval=30\n",
        "        x, y = load_usps('data/usps')\n",
        "    elif args.dataset == 'reutersidf10k':  # recommends: n_clusters=4, update_interval=20\n",
        "        x, y = load_reuters('data/reuters')\n",
        "\n",
        "    # prepare the DEC model\n",
        "    dec = DEC(dims=[x.shape[-1], 500, 500, 2000, 10], n_clusters=args.n_clusters, batch_size=args.batch_size)\n",
        "\n",
        "    dec.initialize_model(optimizer=SGD(lr=0.01, momentum=0.9),\n",
        "                         ae_weights=args.ae_weights)\n",
        "    plot_model(dec.model, to_file='dec_model.png', show_shapes=True)\n",
        "    dec.model.summary()\n",
        "    t0 = time()\n",
        "    y_pred = dec.clustering(x, y=y, tol=args.tol, maxiter=args.maxiter,\n",
        "                            update_interval=args.update_interval, save_dir=args.save_dir)\n",
        "    print 'acc:', cluster_acc(y, y_pred)\n",
        "    print 'clustering time: ', (time() - t0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "dn66JJ87uHKd",
        "outputId": "0463fc3b-c088-4785-f106-acda2f9281fc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-e4c6c0eb04df>\"\u001b[0;36m, line \u001b[0;32m197\u001b[0m\n\u001b[0;31m    logfile = open(save_dir + '/dec\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 197)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from time import time\n",
        "import numpy as np\n",
        "import keras.backend as K\n",
        "from keras.layers import Layer, InputSpec\n",
        "# from keras.utils.layer_utils import InputSpec\n",
        "from keras.layers import Dense, Input\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn import metrics\n",
        "\n",
        "# Define helper functions and classes (from the provided code)\n",
        "# (Including autoencoder, ClusteringLayer, DEC class)\n",
        "\n",
        "# Load the dataset (MNIST)\n",
        "from keras.datasets import mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.reshape(-1, 28 * 28) / 255.0\n",
        "x_test = x_test.reshape(-1, 28 * 28) / 255.0\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "# Define DEC hyperparameters and create the DEC model\n",
        "dims = [x_train.shape[-1], 500, 500, 2000, 10]\n",
        "n_clusters = 10\n",
        "batch_size = 256\n",
        "maxiter = int(2e4)\n",
        "update_interval = 140\n",
        "tol = 0.001\n",
        "\n",
        "# Create and compile the autoencoder model\n",
        "autoencoder = autoencoder(dims=dims)\n",
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Train the autoencoder\n",
        "es = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
        "autoencoder.fit(x_train, x_train, batch_size=batch_size, epochs=100, validation_split=0.1, callbacks=[es])\n",
        "\n",
        "# Initialize the DEC model with autoencoder weights\n",
        "dec = DEC(dims=dims, n_clusters=n_clusters, batch_size=batch_size)\n",
        "dec.initialize_model(optimizer=SGD(lr=0.01, momentum=0.9), ae_weights=None)\n",
        "\n",
        "# Print the model summary\n",
        "dec.model.summary()\n",
        "\n",
        "# Plot the model architecture (optional)\n",
        "plot_model(dec.model, to_file='dec_model.png', show_shapes=True)\n",
        "\n",
        "# Perform clustering on the dataset\n",
        "t0 = time()\n",
        "y_pred = dec.clustering(x_train, y=y_train, tol=tol, maxiter=maxiter, update_interval=update_interval)\n",
        "print('Clustering Time:', (time() - t0))\n",
        "\n",
        "# Evaluate clustering accuracy\n",
        "acc = cluster_acc(y_train, y_pred)\n",
        "print('Clustering Accuracy:', acc)\n",
        "\n",
        "# Save the trained model\n",
        "dec.model.save_weights('mnist_dec_model_final.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "t3qFFtQ1wSwn",
        "outputId": "60558046-c52a-4df4-91f8-0c66bcfe309b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-9ca729585fc0>\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# Create and compile the autoencoder model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mautoencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'autoencoder' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eP6TnA2nuRYR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}